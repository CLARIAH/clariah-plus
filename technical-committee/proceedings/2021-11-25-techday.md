# Aantekeningen en opmerkingen Tech Dag 25-11-2021

## FAIR Datasets

* Eén centrale metadata registry (onderscheid data-metadata explicieter maken), op **dataset** niveau (elders misschien collectie genoemd). Niet op resource niveau.
* Elke dataset moet registreerd zijn (dit zou een requirement zijn)
* 'Sterretjes' of 'Data Readiness Level' per dataset. Met dataset koppeling naar tools die het doorzoekbaar maken op een bepaald niveau van readiness.
* Automatische harvesting nodig (periodiek), Femmy is al bezig met inventarisatie bij partners incl mogelijke endpoints waar data uit te halen is
    * Handmatige registries apart houden (bv WP niveau) en harvesten naar de centrale, aggregatie op centraal niveau
    * incl handmatige JSON-LD schema.org dingen op bv github
    * OAI-PMH wordt binnen CLARIN gebruikt voor metadata harvesting
    * PIDs komen van de downstream repositories
* Welke gemeenschappelijke uitwisselbare standaard kunnen we aannemen? DCAT?
    * DCAT of Schema.org zijn simpel en voldoen. Vgl. [datamodel van NDE Register](https://github.com/netwerk-digitaal-erfgoed/dataset-register#data-model).
    * Vraag: afdwingen dat beschrijvingen gestandaardiseerd worden aangeleverd of mapping onderdeel maken van Registry?
* Doel is vindbaarheid van alle datasets, daarvoor hoeven niet alle details op centraal niveau aanwezig te zijn
* Requirement vanuit Annotation Rolodex: er moet een plek zijn waar individuele onderzoekers persistent collecties zelf kunnen registeren om ernaar te kunnen verwijzen. Dit zelfs als de collectie (nog) niet gedigitaliseerd is. Hier hoort ook een PID bij. Samen: een self-archiving service voor individuen.
* Zoekinterface en high-level integratie met Ineo (vanuit gebruikersperspectief in ieder geval).
* Moeten we dit zelf doen of kunnen we bv meeliften met de VLO? 'registry creep'
    * VLO is alleen linguistisch?
    * Is er een duidelijke behoefte voor een centrale CLARIAH registry? (bv. marketing belang, transparantie binnen project, belang voor andere shared core componenten zoals processing, stimulatie-aspect dat men elkaar's datasets kan vinden, ook cross-domain). Wat is er nodig? 'minimal viable'
* Gebruik bestaande kennis en code (VLO, NDE Register, …).
* Bestaande metadata curatie activiteiten (WP3; MPI, RU, DANS)

## FAIR Annotation

* Drie kanten:
    * Annotatie clients
    * Annotatie opslag (bv elucidate)
    * Anchoring systeem op basis van "canvas" (collecties voorzien van coordinatensystemen)
      (PID op granulariteitsniveau - fragment identifiers)
      vgl IIIF (bv textrepo) (ook relevant voor 3D data)
* Web Annotaties als gemeenschappelijke standaard voor annotatie-uitwisseling, zowel voor handmatig als automatisch
* 'Legacy' bestaande formaten mappen naar Web Annotations?
     * Nadruk op link met FAIR vocabulaires
     * FoLiA, TEI, conversies
     * Koppelingen met bestaande annotatieomgevingen (ELAN) (exports/imports)
     * Text-Fabric as pilot (cuneiform tablets) (Generale Missieven, WP6 usecase 1)
* Provenance aspect is belangrijk onderdeel
* Rolodex


## FAIR Tool Discovery

## FAIR Vocabularies
