# Aantekeningen en opmerkingen Tech Dag 25-11-2021

## FAIR Datasets

* Eén centrale metadata registry (onderscheid data-metadata explicieter maken), op **dataset** niveau (elders misschien collectie genoemd). Niet op resource niveau.
* Elke dataset moet registreerd zijn (dit zou een requirement zijn)
* 'Sterretjes' of 'Data Readiness Level' per dataset. Met dataset koppeling naar tools die het doorzoekbaar maken op een bepaald niveau van readiness.
* Automatische harvesting nodig (periodiek), Femmy is al bezig met inventarisatie bij partners incl mogelijke endpoints waar data uit te halen is
    * Handmatige registries apart houden (bv WP niveau) en harvesten naar de centrale, aggregatie op centraal niveau
    * incl handmatige JSON-LD schema.org dingen op bv github
    * OAI-PMH wordt binnen CLARIN gebruikt voor metadata harvesting
    * PIDs komen van de downstream repositories
* Welke gemeenschappelijke uitwisselbare standaard kunnen we aannemen? DCAT?
    * DCAT of Schema.org zijn simpel en voldoen. Vgl. [datamodel van NDE Register](https://github.com/netwerk-digitaal-erfgoed/dataset-register#data-model).
    * Vraag: afdwingen dat beschrijvingen gestandaardiseerd worden aangeleverd of mapping onderdeel maken van Registry?
* Doel is vindbaarheid van alle datasets, daarvoor hoeven niet alle details op centraal niveau aanwezig te zijn
* Requirement vanuit Annotation Rolodex: er moet een plek zijn waar individuele onderzoekers persistent collecties zelf kunnen registeren om ernaar te kunnen verwijzen. Dit zelfs als de collectie (nog) niet gedigitaliseerd is. Hier hoort ook een PID bij. Samen: een self-archiving service voor individuen.
* Zoekinterface en high-level integratie met Ineo (vanuit gebruikersperspectief in ieder geval).
* Moeten we dit zelf doen of kunnen we bv meeliften met de VLO? 'registry creep'
    * VLO is alleen linguistisch?
    * Is er een duidelijke behoefte voor een centrale CLARIAH registry? (bv. marketing belang, transparantie binnen project, belang voor andere shared core componenten zoals processing, stimulatie-aspect dat men elkaar's datasets kan vinden, ook cross-domain). Wat is er nodig? 'minimal viable'
* Gebruik bestaande kennis en code (VLO, NDE Register, …).
* Bestaande metadata curatie activiteiten (WP3; MPI, RU, DANS)

## FAIR Annotation

* Drie kanten:
    * Annotatie clients
    * Annotatie opslag (bv elucidate)
    * Anchoring systeem op basis van "canvas" (collecties voorzien van coordinatensystemen)
      (PID op granulariteitsniveau - fragment identifiers)
      vgl IIIF (bv textrepo) (ook relevant voor 3D data)
* Web Annotaties als gemeenschappelijke standaard voor annotatie-uitwisseling, zowel voor handmatig als automatisch
* 'Legacy' bestaande formaten mappen naar Web Annotations?
     * Nadruk op link met FAIR vocabulaires
     * FoLiA, TEI, conversies
     * Koppelingen met bestaande annotatieomgevingen (ELAN) (exports/imports)
     * Text-Fabric as pilot (cuneiform tablets) (Generale Missieven, WP6 usecase 1)
* Provenance aspect is belangrijk onderdeel
* Rolodex

## FAIR Vocabularies

* Net als bij FAIR Datasets is deze service een metadata-service die verwijst naar vocabularies, en ze niet zelf aanbiedt.
* Veel bestaand werk in verschillende WPs
* Vocabulary Registry als toevoeging, bindend element tussen bestaand collecties, harvesters, etc
    * BARTOC Skosmos als basis?
    * (dit is 'de gap' die nog ontwikkeld moet worden)
* Crosswalks in één systeem (Relation Registry)
* Versnipperd landschap vanuit verschillende werkpakketen, maar niet heel veel duplicatie behalve Burgerlinker/LenticularLens
* Verband tussen FAIR Datasets and FAIR Vocabularies, is het één CLARIAH+ service? (met meerdere technische onderdelen)
* Motivering: de I = Interoperable van FAIR
* Profileratie risico; niet genoeg hergebruik maar heel veel bijna dubbele-schema's
* Services goed aankaarten bij gebruikers met specificatie, documentatie en training. Als requirement stellen waar nodig

## Data Stories

* Jupyter Notebooks (interactief incl hosting in infrastructuur)
    * ligt voorde hand als de-facto standaard
    * exchange formaat voor data stories (ik (maarten) ben wat skeptisch over de noodzaak hiervan)
* Data stories in Druid (zonder Jupyter Notebooks), proprietary, commercieel
    * Repliceren in een open source variant
* Vraag: is interoperabiliteit met Druid zo belangrijk? kunnen we dat niet droppen?
* JupyterLab ontwikkelt steeds verder in display mogelijkheden: widgets. Die hebben Javascript.
  En die kunnen gerund worden in de browser zonder dat er een kernel runt.
* grote discussie over ontkoppeling data stories en linked open data en te veel onder data stories rekenen
    * Triple stores eerder als provisioning service
    * Duidelijke client APIs aanbieden voor data stories
    * Faciliteit nodig voor mensen die zelf geen triple store willen hosten, triple-store-on-demand
    * waarbij de triples in een triple store ook geserialiseerd kunnen worden naar file en dan elders hergebruikt worden
* tools-to-data perspectief
    * JupyterHub is nodig. Centraal voor CLARIAH? Of van SURF?


## FAIR Tool Discovery

* tool beschrijving in de vorm van codemeta (jsonld + eigen vocab) zo dicht mogelijk bij de tool
* vandaar kunnen toolbeschrijvingen geharvest worden
* toolbeschrijvingen interoperabel maken met die van andere initiatieven, e.g. CLARIN
* discussie: tooldiscovery door eindgebruikers (Ineo) en toolmetadat voor de transparantie van de infrastructuur.
  Dubbel werk doen, of de tooldiscovery afleiden uit de basale metadata?
  Nu nog onduidelijk wat Ineo zelf ophaalt en wat we aan Ineo moeten voorschotelen.
* metadata beschrijvingen voor bestaande tools (door maintainers), werk voor bestaande taken

## Scalable MM Processing => FAIR Distribution and Deployment

* Mario: zorg dat een tool gepackaged kan worden in 1 of meer containers, zodat ze op verschillende plekken kunnen runnen:
  lokaal, op een cluster, in een afgesloten omgeving vlak bij de data
* Roeland: op naar een container registry!
* Jesse: hoe containeriseer je precies? Richtlijnen?
* Dirk: containerisatie is niet triviaal, hoe doe je de security?
* Twee aspecten; provisioning aspect + distributie aspect
    * FAIR Tool Discovery & Distribution?
* Schaalbaarheid: als bij elke minieme code update de build&test pipeline gerund moet worden, kan dat al in de papieren lopen.
    * Kun je ook doen alleen bij getagde releases.
  Het maakt uit of een tool beschikbaar is voor alleen developers of voor alle leden van de federatie.



